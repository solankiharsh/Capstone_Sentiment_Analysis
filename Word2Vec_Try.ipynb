{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec_Try.ipynb",
      "provenance": [],
      "mount_file_id": "12MZMFY-zkVo0Qohuvbuz0y9K_zwfp37q",
      "authorship_tag": "ABX9TyOxQTkw6RJXvQe1tn0HzSWb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solharsh/Capstone_Sentiment_Analysis/blob/master/Word2Vec_Try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6YR2tu6mXrn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "ea231c16-d761-4740-8d9e-c54ba39ebf53"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxxsZbtCmM6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from re import sub\n",
        "import multiprocessing\n",
        "from unidecode import unidecode\n",
        "\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.test.utils import get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "from time import time \n",
        "from collections import defaultdict\n",
        "\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yf-TdUrmUk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = pd.read_csv(\"/content/drive/My Drive/Capstone Project - NLP/Harsh/speech_lemma_cust_stop.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVmgGHtzmn7u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_word_list(text, remove_unnecessary_letters):\n",
        "    ''' Pre process and convert texts to a list of words \n",
        "    method inspired by method from eliorc github repo: https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb'''\n",
        "    text = remove_unnecessary_letters(text)\n",
        "    text = str(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # Clean the text\n",
        "    text = sub(r\"[^A-Za-z0-9^,!?.\\/'+]\", \" \", text)\n",
        "    text = sub(r\"\\+\", \" plus \", text)\n",
        "    text = sub(r\",\", \" \", text)\n",
        "    text = sub(r\"\\.\", \" \", text)\n",
        "    text = sub(r\"!\", \" ! \", text)\n",
        "    text = sub(r\"\\?\", \" ? \", text)\n",
        "    text = sub(r\"'\", \" \", text)\n",
        "    text = sub(r\":\", \" : \", text)\n",
        "    text = sub(r\"\\s{2,}\", \" \", text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftUktIeGmxdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file.Speech_Lemma_Cust_Stop = file.Speech_Lemma_Cust_Stop.apply(lambda x: text_to_word_list(x, unidecode))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8vwuydlnGR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_model = file.copy()\n",
        "file_model = file_model[file_model.Speech_Lemma_Cust_Stop.str.len()>1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC9MNj6vnK97",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fa21d8d-991a-437e-dd21-f844b6c9f75f"
      },
      "source": [
        "sent = [row for row in file_model.Speech_Lemma_Cust_Stop]\n",
        "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[sent]\n",
        "sentences[1]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 10:38:16: collecting all words and their counts\n",
            "INFO - 10:38:16: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "INFO - 10:38:16: collected 9748 word types from a corpus of 11538 words (unigram + bigrams) and 12 sentences\n",
            "INFO - 10:38:16: using 9748 counts as vocab in Phrases<0 vocab, min_count=1, threshold=10.0, max_vocab_size=40000000>\n",
            "INFO - 10:38:16: source_vocab length 9748\n",
            "INFO - 10:38:16: Phraser built with 533 phrasegrams\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['minister',\n",
              " 'decisively',\n",
              " 'status',\n",
              " 'quo',\n",
              " 'poverty_poverty',\n",
              " 'class',\n",
              " 'society',\n",
              " 'decision',\n",
              " 'loss',\n",
              " 'lower',\n",
              " 'concern',\n",
              " 'presently',\n",
              " 'recovery_recovery',\n",
              " 'vis_a',\n",
              " 'vis',\n",
              " 'recovery',\n",
              " 'minister',\n",
              " 'macro_economic',\n",
              " 'lower',\n",
              " 'external',\n",
              " 'comprehensive',\n",
              " 'macro_economic',\n",
              " 'lower',\n",
              " 'deficit',\n",
              " 'current_deficit',\n",
              " 'expect',\n",
              " 'poverty',\n",
              " 'anti',\n",
              " 'poverty',\n",
              " 'reflected',\n",
              " 'minister',\n",
              " 'sab',\n",
              " 'vikas',\n",
              " 'made',\n",
              " 'response',\n",
              " 'inter_generational',\n",
              " 'debt',\n",
              " 'date',\n",
              " 'non',\n",
              " 'tax',\n",
              " 'decline',\n",
              " 'deficit_mainly',\n",
              " 'reduction',\n",
              " 'external',\n",
              " 'turn',\n",
              " 'around',\n",
              " 'ending',\n",
              " 'current_deficit',\n",
              " 'mainly',\n",
              " 'non_essential',\n",
              " 'demand',\n",
              " 'predecessor',\n",
              " 'reducing',\n",
              " 'deficit',\n",
              " 'current',\n",
              " 'low',\n",
              " 'deficit',\n",
              " 'may',\n",
              " 'decided',\n",
              " 'deficit',\n",
              " 'middle',\n",
              " 'east',\n",
              " 'recently',\n",
              " 'problem',\n",
              " 'minimum_maximum',\n",
              " 'governance_operational',\n",
              " 'maximum',\n",
              " 'report',\n",
              " 'sc/sts',\n",
              " 'scheme',\n",
              " 'power',\n",
              " 'keeping_mind',\n",
              " 'honble',\n",
              " 'consequent',\n",
              " 'conclusion',\n",
              " 'juncture',\n",
              " 'predictable',\n",
              " 'keeping_mind',\n",
              " 'decided',\n",
              " 'respect',\n",
              " 'demand',\n",
              " 'reduce',\n",
              " 'non_resident',\n",
              " 'respect',\n",
              " 'defined',\n",
              " 'strengthen',\n",
              " 'income_tax',\n",
              " 'may',\n",
              " 'custom',\n",
              " 'part_b',\n",
              " 'promote',\n",
              " 'resource',\n",
              " 'job_job',\n",
              " 'job',\n",
              " 'defence_defence',\n",
              " 'private',\n",
              " 'defence_defence',\n",
              " 'current',\n",
              " 'neo_middle',\n",
              " 'class',\n",
              " 'requirement',\n",
              " 'capital',\n",
              " 'reduced',\n",
              " 'respectively',\n",
              " 'post',\n",
              " 'project',\n",
              " 'low',\n",
              " 'minimum',\n",
              " 'condition',\n",
              " 'lock_in',\n",
              " 'e_commerce',\n",
              " 'recovery',\n",
              " 'basel',\n",
              " 'iii',\n",
              " 'requirement',\n",
              " 'capital',\n",
              " 'requirement',\n",
              " 'capital',\n",
              " 'capital',\n",
              " 'capital',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'class',\n",
              " 'soon',\n",
              " 'minister',\n",
              " 'one',\n",
              " 'cities',\n",
              " 'mid',\n",
              " 'sized',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'e_visa',\n",
              " 'job',\n",
              " 'e_visa',\n",
              " 'place',\n",
              " 'extended',\n",
              " 'purpose',\n",
              " 'reduce',\n",
              " 'pressure',\n",
              " 'may',\n",
              " 'multi',\n",
              " 'skill',\n",
              " 'support',\n",
              " 'scheme',\n",
              " 'pradhan_yojana',\n",
              " 'sum_purpose',\n",
              " 'support',\n",
              " 'mahatma',\n",
              " 'project',\n",
              " 'scheme',\n",
              " 'power',\n",
              " 'power',\n",
              " 'supply',\n",
              " 'deen',\n",
              " 'dayal',\n",
              " 'yojana',\n",
              " 'power',\n",
              " 'supply',\n",
              " 'sub',\n",
              " 'transmission',\n",
              " 'sum_purpose',\n",
              " 'support',\n",
              " 'sum',\n",
              " 'governments',\n",
              " 'strengthen',\n",
              " 'resolve',\n",
              " 'castes/scheduled',\n",
              " 'neo_middle',\n",
              " 'class',\n",
              " 'sum',\n",
              " 'scheme',\n",
              " 'van',\n",
              " 'yojana',\n",
              " 'scheme',\n",
              " 'scheme',\n",
              " 'scheme',\n",
              " 'lying',\n",
              " 'post_saving',\n",
              " 'payment',\n",
              " 'report',\n",
              " 'social',\n",
              " 'minimum',\n",
              " 'scheme',\n",
              " 'made',\n",
              " 'current',\n",
              " 'made',\n",
              " 'current',\n",
              " 'uniform',\n",
              " 'number',\n",
              " 'society',\n",
              " 'extend',\n",
              " 'scheme',\n",
              " 'purchase/fitting',\n",
              " 'private',\n",
              " 'demand',\n",
              " 'current',\n",
              " 'currency',\n",
              " 'womens',\n",
              " 'concern',\n",
              " 'scaled',\n",
              " 'ministry',\n",
              " 'pilot',\n",
              " 'scheme',\n",
              " 'safety',\n",
              " 'transport',\n",
              " 'sum',\n",
              " 'ministry',\n",
              " 'scheme',\n",
              " 'crisis',\n",
              " 'centres',\n",
              " 'private',\n",
              " 'scheme',\n",
              " 'sum',\n",
              " 'process',\n",
              " 'nda_i',\n",
              " 'minister',\n",
              " 'minister',\n",
              " 'sum',\n",
              " 'self_employment',\n",
              " 'poverty',\n",
              " 'prompt',\n",
              " 'extend',\n",
              " 'start',\n",
              " 'entrepreneurship',\n",
              " 'programme',\n",
              " 'entrepreneurship',\n",
              " 'sum',\n",
              " 'scheme',\n",
              " 'expand',\n",
              " 'support',\n",
              " 'neeranchal',\n",
              " 'current',\n",
              " 'capacity',\n",
              " 'panchayats/',\n",
              " 'intra',\n",
              " 'district',\n",
              " 'sub',\n",
              " 'districts',\n",
              " 'receive',\n",
              " 'support',\n",
              " 'heavy/',\n",
              " 'pesticides/',\n",
              " 'health',\n",
              " 'all',\n",
              " 'key',\n",
              " 'i_e',\n",
              " 'priority',\n",
              " 'sum',\n",
              " 'presently',\n",
              " 'strengthen',\n",
              " 'states',\n",
              " 'keeping',\n",
              " 'governments',\n",
              " 'care',\n",
              " 'concerning',\n",
              " 'pandit',\n",
              " 'programme',\n",
              " 'sum',\n",
              " 'sum',\n",
              " 'class',\n",
              " 'sum',\n",
              " 'haves',\n",
              " 'have',\n",
              " 'nots',\n",
              " 'digital',\n",
              " 'india',\n",
              " 'connectivity',\n",
              " 'production',\n",
              " 'product',\n",
              " 'e',\n",
              " 'kranti',\n",
              " 'scheme',\n",
              " 'sum_purpose',\n",
              " 'good',\n",
              " 'governance',\n",
              " 'sum',\n",
              " 'scheme',\n",
              " 'scheme',\n",
              " 'support',\n",
              " 'film',\n",
              " 'film',\n",
              " 'national',\n",
              " 'physical_connectivity',\n",
              " 'support',\n",
              " 'private',\n",
              " 'capital',\n",
              " 'debt_debt',\n",
              " 'promote',\n",
              " 'keeping',\n",
              " 'honble',\n",
              " 'ministers',\n",
              " 'extension',\n",
              " 'plus',\n",
              " 'current',\n",
              " 'sum_purpose',\n",
              " 'extend',\n",
              " 'low_low',\n",
              " 'sum',\n",
              " 'poor/ews/lig',\n",
              " 'social',\n",
              " 'private',\n",
              " 'comprehensive',\n",
              " 'place',\n",
              " 'up',\n",
              " 'goods',\n",
              " 'preserve',\n",
              " '/th',\n",
              " 'self_sufficient',\n",
              " 'private_agro',\n",
              " 'technology',\n",
              " 'agri',\n",
              " 'business',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'agri',\n",
              " 'tech',\n",
              " 'fund_sum',\n",
              " 'purpose',\n",
              " 'soil',\n",
              " 'concern',\n",
              " 'scheme',\n",
              " 'soil_card',\n",
              " 'sum_purpose',\n",
              " 'soil_soil',\n",
              " 'face',\n",
              " 'national',\n",
              " 'fund_sum',\n",
              " 'productivity',\n",
              " 'protein',\n",
              " 'revolution',\n",
              " 'lenders',\n",
              " 'joint',\n",
              " 'bhoomi',\n",
              " 'kisan',\n",
              " 'current',\n",
              " 'produce',\n",
              " 'sum',\n",
              " 'consumers',\n",
              " 're',\n",
              " 'orient',\n",
              " 'private',\n",
              " 'yards/',\n",
              " 'private',\n",
              " 'farmers_produce',\n",
              " 'sum',\n",
              " 'support',\n",
              " 'subvention',\n",
              " 'scheme',\n",
              " 'subvention',\n",
              " 'scheme',\n",
              " 'extending',\n",
              " 'concessional',\n",
              " 'scheme',\n",
              " 'priority',\n",
              " 'current',\n",
              " 'capacity_capacity',\n",
              " 'keeping',\n",
              " 'scientific',\n",
              " 'long',\n",
              " 'fund',\n",
              " 'purpose',\n",
              " 'support',\n",
              " 'produce',\n",
              " 'nabards',\n",
              " 'producers',\n",
              " 'producers',\n",
              " 'produce',\n",
              " 'sum',\n",
              " 'reducing',\n",
              " 'priority',\n",
              " 'society',\n",
              " 'rainfall',\n",
              " 'decline',\n",
              " 'production',\n",
              " 'keep',\n",
              " 'current',\n",
              " 'sum_purpose',\n",
              " 'payment',\n",
              " 'priority',\n",
              " 'connectivity',\n",
              " 'indias',\n",
              " 'purpose',\n",
              " 'chennai',\n",
              " 'bengaluru',\n",
              " 'vizag',\n",
              " 'chennai',\n",
              " 'key',\n",
              " 'promotion',\n",
              " 'indias',\n",
              " 'promotion',\n",
              " 'production',\n",
              " 'promotion',\n",
              " 'investors',\n",
              " 'scheme',\n",
              " 'scheme',\n",
              " 'micro',\n",
              " 'ministry_ministry',\n",
              " 'promotion_entrepreneurship',\n",
              " 'start_up',\n",
              " 'start_up',\n",
              " 'capital',\n",
              " 'conducive',\n",
              " 'eco_system',\n",
              " 'capital',\n",
              " 'private',\n",
              " 'capital',\n",
              " 'capital',\n",
              " 'start_up',\n",
              " 'promote_entrepreneurship',\n",
              " 'agro',\n",
              " 'industry',\n",
              " 'definition',\n",
              " 'capital',\n",
              " 'place',\n",
              " 'framework',\n",
              " 'district',\n",
              " 'programme',\n",
              " 'support',\n",
              " 'entrepreneurship',\n",
              " 'promote',\n",
              " 'support',\n",
              " 'mega',\n",
              " 'cluster',\n",
              " 'mega',\n",
              " 'clusters',\n",
              " 'sum_purpose',\n",
              " 'preservation',\n",
              " 'handloom/handicraft',\n",
              " 'sum_purpose',\n",
              " 'promotion',\n",
              " 'sum_purpose',\n",
              " 'framework',\n",
              " 'support',\n",
              " 'connectivity',\n",
              " 'project',\n",
              " 'comprehensive',\n",
              " 'promote',\n",
              " 'current',\n",
              " 'capacity',\n",
              " 'project',\n",
              " 'jal',\n",
              " 'vikas',\n",
              " 'waterways',\n",
              " 'i',\n",
              " 'project',\n",
              " 'connectivity',\n",
              " 'scheme',\n",
              " 'nda_i',\n",
              " 'supply',\n",
              " 'project',\n",
              " 'sum',\n",
              " 'power',\n",
              " 'promote',\n",
              " 'power',\n",
              " 'sum',\n",
              " 'scheme',\n",
              " 'ultra',\n",
              " 'super_power',\n",
              " 'technology',\n",
              " 'comprehensive',\n",
              " 'production',\n",
              " 'place',\n",
              " 'supply',\n",
              " 'power',\n",
              " 'reduce',\n",
              " 'power',\n",
              " 'renewable_energy',\n",
              " 'renewable_energy',\n",
              " 'priority',\n",
              " 'solar_power',\n",
              " 'sum',\n",
              " 'scheme',\n",
              " 'solar_power',\n",
              " 'sum_purpose',\n",
              " 'solar_energy',\n",
              " 'project',\n",
              " 'renewable_energy',\n",
              " 'governments',\n",
              " 'production',\n",
              " 'production',\n",
              " 'scaled',\n",
              " 'clean',\n",
              " 'pipeline',\n",
              " 'long_term',\n",
              " 'reducing_dependence',\n",
              " 'energy',\n",
              " 'mining',\n",
              " 'governments',\n",
              " 'mining',\n",
              " 'promote',\n",
              " 'mining',\n",
              " 'current',\n",
              " 'mining',\n",
              " 'mining',\n",
              " 'honble',\n",
              " 'place',\n",
              " 'capital',\n",
              " 'external',\n",
              " 'resource',\n",
              " 'recent',\n",
              " 'strengthen',\n",
              " 'framework',\n",
              " 'process',\n",
              " 'framework',\n",
              " 'place',\n",
              " 'framework',\n",
              " 'deep',\n",
              " 'deepen',\n",
              " 'currency',\n",
              " 'extend',\n",
              " 'extend',\n",
              " 'scheme',\n",
              " 'adr/gdr',\n",
              " 'depository',\n",
              " 'debt',\n",
              " 'depository_depository',\n",
              " 'resolve',\n",
              " 'long',\n",
              " 'standing',\n",
              " 'problem',\n",
              " 'capital',\n",
              " 'capital',\n",
              " 'inter',\n",
              " 'usability',\n",
              " 'framework',\n",
              " 'post_harvest',\n",
              " 'current',\n",
              " 'separately',\n",
              " 'date',\n",
              " 'separately',\n",
              " 'society',\n",
              " 'private',\n",
              " 'extend',\n",
              " 'potential',\n",
              " 'minimum',\n",
              " 'priority',\n",
              " 'current',\n",
              " 'framework',\n",
              " 'place',\n",
              " 'private',\n",
              " 'current',\n",
              " 'framework',\n",
              " 'payment',\n",
              " 'low',\n",
              " 'migrant',\n",
              " 'concern',\n",
              " 'debt_recovery',\n",
              " 'low',\n",
              " 'multi_pronged',\n",
              " 'support',\n",
              " 'concerned',\n",
              " 'micro',\n",
              " 'offices',\n",
              " 'scheme',\n",
              " 'decline',\n",
              " 'scheme',\n",
              " 'p_a',\n",
              " 'defence_defence',\n",
              " 'current',\n",
              " 'defence',\n",
              " 'one',\n",
              " 'pension',\n",
              " 'sum_years',\n",
              " 'requirement',\n",
              " 'defence',\n",
              " 'indias',\n",
              " 'capital',\n",
              " 'defence',\n",
              " 'sum',\n",
              " 'procurement',\n",
              " 'process',\n",
              " 'deeply',\n",
              " 'made',\n",
              " 'defend',\n",
              " 'sum_purpose',\n",
              " 'defence',\n",
              " 'production',\n",
              " 'private',\n",
              " 'scientific',\n",
              " 'support',\n",
              " 'defence',\n",
              " 'cutting',\n",
              " 'edge',\n",
              " 'capability',\n",
              " 'sum',\n",
              " 'support',\n",
              " 'scheme',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'strengthen',\n",
              " 'sum',\n",
              " 'sum',\n",
              " 'sum',\n",
              " 'ear',\n",
              " 'marked',\n",
              " 'process',\n",
              " 'sum_purpose',\n",
              " 'current',\n",
              " 'indias_potential',\n",
              " 'job',\n",
              " 'sum_purpose',\n",
              " 'sum_purpose',\n",
              " 'sum_purpose',\n",
              " 'project',\n",
              " 'preservation',\n",
              " 'purpose',\n",
              " 'sum',\n",
              " 'sarnath',\n",
              " 'gaya',\n",
              " 'varanasi',\n",
              " 'class',\n",
              " 'declared',\n",
              " 'film',\n",
              " 'class',\n",
              " 'private',\n",
              " 'support',\n",
              " 'scheme',\n",
              " 'made',\n",
              " 'project',\n",
              " 'sum',\n",
              " 'place',\n",
              " 'namami',\n",
              " 'gange',\n",
              " 'sum_purpose',\n",
              " 'process',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'process',\n",
              " 'preservation',\n",
              " 'science_science',\n",
              " 'countrys',\n",
              " 'leading',\n",
              " 'science',\n",
              " 'strengthen',\n",
              " 'private',\n",
              " 'scaled',\n",
              " 'model',\n",
              " 'high_end',\n",
              " 'agri',\n",
              " 'biotech',\n",
              " 'scaled',\n",
              " 'plant',\n",
              " 'genetic',\n",
              " 'private',\n",
              " 'indias',\n",
              " 'world',\n",
              " 'leader',\n",
              " 'indias',\n",
              " 'capacity',\n",
              " 'mk',\n",
              " 'iii',\n",
              " 'helio',\n",
              " 'centric',\n",
              " 'main',\n",
              " 'streamed',\n",
              " 'date',\n",
              " 'sports',\n",
              " 'academies',\n",
              " 'sub',\n",
              " 'junior',\n",
              " 'lot',\n",
              " 'sum',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'promote',\n",
              " 'promote',\n",
              " 'sum',\n",
              " 'job',\n",
              " 'extend',\n",
              " 'sum_purpose',\n",
              " 'pragmatic',\n",
              " 'promote',\n",
              " 'programme',\n",
              " 'require',\n",
              " 'support',\n",
              " 'sum',\n",
              " 'current',\n",
              " 'capacity',\n",
              " 'custom',\n",
              " 'custom',\n",
              " 'potential',\n",
              " 'demand',\n",
              " 'sum_purpose',\n",
              " 'current',\n",
              " 'connectivity_connectivity',\n",
              " 'connectivity',\n",
              " 'purpose',\n",
              " 'sum',\n",
              " 'powerful',\n",
              " 'north',\n",
              " 'arun',\n",
              " 'prabha',\n",
              " 're',\n",
              " 'organization',\n",
              " 'made',\n",
              " 'ministries/departments',\n",
              " 'capital',\n",
              " 'in',\n",
              " 'migration',\n",
              " 'supply',\n",
              " 'class',\n",
              " 'power',\n",
              " 'supply',\n",
              " 'capital',\n",
              " 'dam',\n",
              " 'priority',\n",
              " 'sum',\n",
              " 'sum',\n",
              " 'main',\n",
              " 'keeping_mind',\n",
              " 'prepared',\n",
              " 'non_plan',\n",
              " 'non_plan',\n",
              " 'care',\n",
              " 'capital',\n",
              " 'capacity',\n",
              " 'main',\n",
              " 'capacity',\n",
              " 'energy',\n",
              " 'current',\n",
              " 'capital',\n",
              " 'deficit_deficit',\n",
              " 'honble',\n",
              " 'made',\n",
              " 'compulsory',\n",
              " 'made',\n",
              " 'current',\n",
              " 'separately',\n",
              " 'made',\n",
              " 'made',\n",
              " 'made',\n",
              " 'predecessor',\n",
              " 'promote',\n",
              " 'reduce',\n",
              " 'problem',\n",
              " 'main',\n",
              " 'income_tax',\n",
              " 'concern',\n",
              " 'lower',\n",
              " 'class',\n",
              " 'reduce',\n",
              " 'deduction',\n",
              " 'respect',\n",
              " 'scale',\n",
              " 'conducive',\n",
              " 'i_e',\n",
              " 'scheme',\n",
              " 'extend',\n",
              " 'deduction',\n",
              " 'supply',\n",
              " 'power',\n",
              " 'concern',\n",
              " 'extend',\n",
              " 'power',\n",
              " 'presence',\n",
              " 'may',\n",
              " 'capital',\n",
              " 'concessional',\n",
              " 'received',\n",
              " 'concessional',\n",
              " 'date',\n",
              " 'low',\n",
              " 'extend',\n",
              " 'date',\n",
              " 'currency',\n",
              " 'concessional',\n",
              " 'extend',\n",
              " 'reduce',\n",
              " 'scheme',\n",
              " 'received_response',\n",
              " 'strengthen',\n",
              " 'roll',\n",
              " 'back',\n",
              " 'scheme',\n",
              " 'may',\n",
              " 'concept',\n",
              " 'arms',\n",
              " 'concept',\n",
              " 'data_data',\n",
              " 'data',\n",
              " 'income_tax',\n",
              " 'current',\n",
              " 'capital',\n",
              " 'concessional',\n",
              " 'debt',\n",
              " 'capital',\n",
              " 'respect',\n",
              " 'purpose',\n",
              " 'deduct',\n",
              " 'deduction',\n",
              " 'report',\n",
              " 'predecessor',\n",
              " 'received',\n",
              " 'income_tax',\n",
              " 'extend',\n",
              " 'current',\n",
              " 'promote',\n",
              " 'loss',\n",
              " 'custom',\n",
              " 'reduce',\n",
              " 'custom',\n",
              " 'oleo',\n",
              " 'chemicals',\n",
              " 'pitch',\n",
              " 'capacity',\n",
              " 'reduce',\n",
              " 'custom',\n",
              " 'ortho',\n",
              " 'xylene',\n",
              " 'demand',\n",
              " 'production',\n",
              " 'reduce_dependence',\n",
              " 'custom',\n",
              " 'inputs/components',\n",
              " 'produced',\n",
              " 'custom',\n",
              " 'concession',\n",
              " 'production',\n",
              " 'reduce',\n",
              " 'custom',\n",
              " 'custom',\n",
              " 'presently',\n",
              " 'under',\n",
              " 'utilization',\n",
              " 'capacity',\n",
              " 'custom',\n",
              " 'flat_rolled',\n",
              " 'solar_power',\n",
              " 'solar',\n",
              " 'custom',\n",
              " 'concessional',\n",
              " 'custom',\n",
              " 'extended',\n",
              " 'project',\n",
              " 'solar_energy',\n",
              " 'production',\n",
              " 'promote',\n",
              " 'energy',\n",
              " 'reduce',\n",
              " 'custom',\n",
              " 'concessional',\n",
              " 'custom',\n",
              " 'bio_cng',\n",
              " 'custom',\n",
              " 'non',\n",
              " 'agglomerated',\n",
              " 'custom',\n",
              " 'custom',\n",
              " 'custom',\n",
              " 'custom',\n",
              " 'reducing',\n",
              " 'custom',\n",
              " 'semi_processed',\n",
              " 'presently',\n",
              " 'custom',\n",
              " 'custom',\n",
              " 'mis',\n",
              " 'use',\n",
              " 'custom',\n",
              " 'semi_processed',\n",
              " 'pre_forms',\n",
              " 'precious_semi',\n",
              " 'precious',\n",
              " 'custom',\n",
              " 'capital',\n",
              " 'extended',\n",
              " 'june',\n",
              " 'expect',\n",
              " 'production',\n",
              " 'post',\n",
              " 'produce',\n",
              " 'mainly',\n",
              " 'capacity_capacity',\n",
              " 'reduce',\n",
              " 'reduce',\n",
              " 'concessional',\n",
              " 'renewable_energy',\n",
              " 'solar_solar',\n",
              " 'solar_solar',\n",
              " 'project',\n",
              " 'solar_energy',\n",
              " 'production',\n",
              " 'bio_cng',\n",
              " 'on_going',\n",
              " 'june',\n",
              " 'may',\n",
              " 'concessional_sugar',\n",
              " 'energy',\n",
              " 'presently',\n",
              " 'energy_energy',\n",
              " 'expand',\n",
              " 'energy',\n",
              " 'recent',\n",
              " 'kept',\n",
              " 'extent',\n",
              " 'extended',\n",
              " 'radio',\n",
              " 'taxis',\n",
              " 'place',\n",
              " 'rent_a',\n",
              " 'cab',\n",
              " 'date',\n",
              " 'extended',\n",
              " 'air_conditioned',\n",
              " 'place',\n",
              " 'reduced',\n",
              " 'response',\n",
              " 'request',\n",
              " 'demand',\n",
              " 'rent_a',\n",
              " 'cab',\n",
              " 'social',\n",
              " 'exemption',\n",
              " 'induced',\n",
              " 'request',\n",
              " 'ministry',\n",
              " 'produce',\n",
              " 'employees',\n",
              " 'presently',\n",
              " 'micro',\n",
              " 'expanded_micro',\n",
              " 'sum',\n",
              " 'bio',\n",
              " 'medical',\n",
              " 'reducing',\n",
              " 'custom',\n",
              " 'reflected',\n",
              " 'clearance',\n",
              " 'extend',\n",
              " 'custom',\n",
              " 'clearance',\n",
              " 'respect',\n",
              " 'respect',\n",
              " 'indian',\n",
              " 'custom',\n",
              " 'project',\n",
              " 'clearance',\n",
              " 'reduce',\n",
              " 'scheme',\n",
              " 'expanded',\n",
              " 'private',\n",
              " 'respect',\n",
              " 'resolution',\n",
              " 'process',\n",
              " 'custom',\n",
              " 'final']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmAefA4cnWdp",
        "colab_type": "text"
      },
      "source": [
        "min count = 3 - remove most unusual words from training embeddings, like words 'promote_entrepreneurship', which actually stands for 'promote entrepreneaurship', and doesn't need additional training\n",
        "\n",
        "- window = 4 - Word2Vec model will learn to predict given word from up to 4 words to the left, and up to 4 words to the right\n",
        "\n",
        "- size = 300 - size of hidden layer used to predict surroundings of embedded word, which also stands for dimensions of trained embeddings\n",
        "\n",
        "- sample = 1e-5 - probability baseline for subsampling most frequent words from surrounding of embedded word\n",
        "\n",
        "- negative = 20 - number of negative (ones that shouldn't have been predicted while modeling selected pair of words) words that will have their corresponding weights updated while training on specific training example, along with positive word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCa_8AofnQ-F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "a1db5587-15fa-43df-978f-3667d9fe3b7a"
      },
      "source": [
        "w2v_model = Word2Vec(min_count=3,\n",
        "                     window=4,\n",
        "                     size=300,\n",
        "                     sample=1e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=multiprocessing.cpu_count()-1)\n",
        "\n",
        "start = time()\n",
        "\n",
        "w2v_model.build_vocab(sentences, progress_per=50000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - start) / 60, 2)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 10:39:43: collecting all words and their counts\n",
            "INFO - 10:39:43: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "INFO - 10:39:43: collected 1948 word types from a corpus of 9936 raw words and 12 sentences\n",
            "INFO - 10:39:43: Loading a fresh vocabulary\n",
            "INFO - 10:39:43: effective_min_count=3 retains 545 unique words (27% of original 1948, drops 1403)\n",
            "INFO - 10:39:43: effective_min_count=3 leaves 8124 word corpus (81% of original 9936, drops 1812)\n",
            "INFO - 10:39:43: deleting the raw counts dictionary of 1948 items\n",
            "INFO - 10:39:43: sample=1e-05 downsamples 545 most-common words\n",
            "INFO - 10:39:43: downsampling leaves estimated 535 word corpus (6.6% of prior 8124)\n",
            "INFO - 10:39:43: estimated required memory for 545 words and 300 dimensions: 1580500 bytes\n",
            "INFO - 10:39:43: resetting layer weights\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to build vocab: 0.0 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwdt8xNFnmBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4d5c9da3-d46a-4d54-ad1f-e4cbaf15ba3a"
      },
      "source": [
        "start = time()\n",
        "\n",
        "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - start) / 60, 2)))\n",
        "\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 10:39:53: training model with 1 workers on 545 vocabulary and 300 features, using sg=0 hs=0 sample=1e-05 negative=20 window=4\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 1 : training on 9936 raw words (499 effective words) took 0.0s, 14276 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 2 : training on 9936 raw words (543 effective words) took 0.0s, 16513 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 3 : training on 9936 raw words (548 effective words) took 0.0s, 19831 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 4 : training on 9936 raw words (506 effective words) took 0.0s, 18827 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 5 : training on 9936 raw words (586 effective words) took 0.0s, 20129 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 6 : training on 9936 raw words (568 effective words) took 0.0s, 18512 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 7 : training on 9936 raw words (533 effective words) took 0.0s, 18025 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 8 : training on 9936 raw words (516 effective words) took 0.0s, 17599 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 9 : training on 9936 raw words (528 effective words) took 0.0s, 11919 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 10 : training on 9936 raw words (526 effective words) took 0.0s, 17313 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 11 : training on 9936 raw words (543 effective words) took 0.0s, 16922 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 12 : training on 9936 raw words (555 effective words) took 0.0s, 19562 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 13 : training on 9936 raw words (536 effective words) took 0.0s, 17446 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 14 : training on 9936 raw words (532 effective words) took 0.0s, 17644 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 15 : training on 9936 raw words (548 effective words) took 0.0s, 19068 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 16 : training on 9936 raw words (528 effective words) took 0.0s, 18031 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 17 : training on 9936 raw words (557 effective words) took 0.0s, 20305 effective words/s\n",
            "INFO - 10:39:53: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:53: EPOCH - 18 : training on 9936 raw words (553 effective words) took 0.0s, 18624 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 19 : training on 9936 raw words (555 effective words) took 0.0s, 18476 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 20 : training on 9936 raw words (551 effective words) took 0.0s, 18615 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 21 : training on 9936 raw words (531 effective words) took 0.0s, 17309 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 22 : training on 9936 raw words (547 effective words) took 0.0s, 18882 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 23 : training on 9936 raw words (512 effective words) took 0.0s, 15429 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 24 : training on 9936 raw words (547 effective words) took 0.0s, 18559 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 25 : training on 9936 raw words (550 effective words) took 0.0s, 20403 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 26 : training on 9936 raw words (531 effective words) took 0.0s, 19608 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 27 : training on 9936 raw words (527 effective words) took 0.0s, 19619 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 28 : training on 9936 raw words (513 effective words) took 0.0s, 14983 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 29 : training on 9936 raw words (509 effective words) took 0.0s, 17098 effective words/s\n",
            "INFO - 10:39:54: worker thread finished; awaiting finish of 0 more threads\n",
            "INFO - 10:39:54: EPOCH - 30 : training on 9936 raw words (566 effective words) took 0.0s, 20022 effective words/s\n",
            "INFO - 10:39:54: training on a 298080 raw words (16144 effective words) took 1.2s, 13779 effective words/s\n",
            "INFO - 10:39:54: precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to train the model: 0.02 mins\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdZsGPsUnohv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "67129bd3-f003-4a19-a99a-874c42326e3d"
      },
      "source": [
        "w2v_model.save(\"word2vec_another.model\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 10:40:09: saving Word2Vec object under word2vec_another.model, separately None\n",
            "INFO - 10:40:09: not storing attribute vectors_norm\n",
            "INFO - 10:40:09: not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "INFO - 10:40:09: saved word2vec_another.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxWuzX5Dnsba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_export = file_model.copy()\n",
        "file_export['Speech_Lemma_Cust_Stop'] = file_export.Speech_Lemma_Cust_Stop\n",
        "file_export.Speech_Lemma_Cust_Stop = file_export.Speech_Lemma_Cust_Stop.apply(lambda x: ' '.join(bigram[x]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOtSRTP0pVSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_export.to_csv('cleaned_dataset.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PXtisOKoG4-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2622b19d-8cf1-4f5c-b23a-7d017b190354"
      },
      "source": [
        "word_vectors = Word2Vec.load(\"/content/word2vec_another.model\").wv\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 10:42:13: loading Word2Vec object from /content/word2vec_another.model\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "INFO - 10:42:13: loading wv recursively from /content/word2vec_another.model.wv.* with mmap=None\n",
            "INFO - 10:42:13: setting ignored attribute vectors_norm to None\n",
            "INFO - 10:42:13: loading vocabulary recursively from /content/word2vec_another.model.vocabulary.* with mmap=None\n",
            "INFO - 10:42:13: loading trainables recursively from /content/word2vec_another.model.trainables.* with mmap=None\n",
            "INFO - 10:42:13: setting ignored attribute cum_table to None\n",
            "INFO - 10:42:13: loaded /content/word2vec_another.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szgVKkoHoKqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvCi43aJoMCA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "dbd38c29-4a07-4e36-93bc-e01a64c43a37"
      },
      "source": [
        "word_vectors.similar_by_vector(model.cluster_centers_[0], topn=10, restrict_vocab=None)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 10:42:36: precomputing L2-norms of word weight vectors\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('super', 0.9999998807907104),\n",
              " ('lock_in', 0.9994293451309204),\n",
              " ('non_performing', 0.9994256496429443),\n",
              " ('e', 0.9994200468063354),\n",
              " ('external', 0.9994189739227295),\n",
              " ('class', 0.9994183778762817),\n",
              " ('private', 0.9994181394577026),\n",
              " ('produce', 0.9994180202484131),\n",
              " ('film', 0.9994176626205444),\n",
              " ('sum', 0.9994176030158997)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLblYa_goQZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_cluster_center = model.cluster_centers_[0]\n",
        "negative_cluster_center = model.cluster_centers_[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5evLbDcoUhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "43c9561f-c49b-4baa-f1b1-3ff04860f07f"
      },
      "source": [
        "words = pd.DataFrame(word_vectors.vocab.keys())\n",
        "words.columns = ['words']\n",
        "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
        "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
        "words.cluster = words.cluster.apply(lambda x: x[0])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Yle3vlodzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "1660e45d-07f6-4019-eab4-40f2f35e1b92"
      },
      "source": [
        "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
        "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
        "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  \n",
            "INFO - 10:43:39: NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzVgb-bCoftg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "6534394e-d472-49f3-f757-2692eff1a784"
      },
      "source": [
        "words.head(10)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>vectors</th>\n",
              "      <th>cluster</th>\n",
              "      <th>cluster_value</th>\n",
              "      <th>closeness_score</th>\n",
              "      <th>sentiment_coeff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>minister</td>\n",
              "      <td>[-0.00030517866, -0.07556432, -0.0051926235, -...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>239.548713</td>\n",
              "      <td>-239.548713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>recovery</td>\n",
              "      <td>[0.00058132334, -0.0756106, -0.005140628, -0.0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>184.170159</td>\n",
              "      <td>-184.170159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>debt</td>\n",
              "      <td>[0.0002607753, -0.075390615, -0.0053126127, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>183.606724</td>\n",
              "      <td>-183.606724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ones</td>\n",
              "      <td>[0.0014946946, -0.07712085, -0.0033782392, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>66.317436</td>\n",
              "      <td>-66.317436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>indias</td>\n",
              "      <td>[0.00039181928, -0.07590666, -0.004629179, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>269.540627</td>\n",
              "      <td>-269.540627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>product</td>\n",
              "      <td>[-0.0005033336, -0.076035015, -0.0046590795, -...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>182.178554</td>\n",
              "      <td>-182.178554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>preceding</td>\n",
              "      <td>[0.0006061077, -0.07523416, -0.0047281403, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>150.709033</td>\n",
              "      <td>-150.709033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>years</td>\n",
              "      <td>[0.0012034712, -0.074813545, -0.006155806, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>73.126230</td>\n",
              "      <td>-73.126230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>response</td>\n",
              "      <td>[-0.0006459907, -0.07620716, -0.004064309, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>109.224104</td>\n",
              "      <td>-109.224104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>expanded</td>\n",
              "      <td>[0.0003275104, -0.076102555, -0.005201346, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>166.405656</td>\n",
              "      <td>-166.405656</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       words  ... sentiment_coeff\n",
              "0   minister  ...     -239.548713\n",
              "1   recovery  ...     -184.170159\n",
              "2       debt  ...     -183.606724\n",
              "3       ones  ...      -66.317436\n",
              "4     indias  ...     -269.540627\n",
              "5    product  ...     -182.178554\n",
              "6  preceding  ...     -150.709033\n",
              "7      years  ...      -73.126230\n",
              "8   response  ...     -109.224104\n",
              "9   expanded  ...     -166.405656\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOlyGbiGqUPu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "fa3ad159-24ab-4966-e6f1-e5b9083c5de1"
      },
      "source": [
        "words.tail(10)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>vectors</th>\n",
              "      <th>cluster</th>\n",
              "      <th>cluster_value</th>\n",
              "      <th>closeness_score</th>\n",
              "      <th>sentiment_coeff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>sub_plan</td>\n",
              "      <td>[-0.00039711164, -0.07526852, -0.0048797587, -...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>148.799847</td>\n",
              "      <td>-148.799847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>across</td>\n",
              "      <td>[-6.605081e-05, -0.076037645, -0.004618882, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>167.555526</td>\n",
              "      <td>-167.555526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>w_e</td>\n",
              "      <td>[-0.00051648536, -0.07572093, -0.005264799, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>80.940936</td>\n",
              "      <td>-80.940936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>f</td>\n",
              "      <td>[0.00013480373, -0.07438007, -0.0051311315, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>66.701085</td>\n",
              "      <td>-66.701085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>charger/adapter_headsets/speakers</td>\n",
              "      <td>[0.00074677466, -0.07657748, -0.005479288, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>99.961028</td>\n",
              "      <td>-99.961028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>set_top</td>\n",
              "      <td>[-0.0006023316, -0.075583525, -0.004217541, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>149.250401</td>\n",
              "      <td>-149.250401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>repair_condition</td>\n",
              "      <td>[0.0006244523, -0.075133726, -0.0052383034, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>116.650753</td>\n",
              "      <td>-116.650753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>pile_epingle</td>\n",
              "      <td>[0.00068346446, -0.07575631, -0.0057524047, -0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>106.838911</td>\n",
              "      <td>-106.838911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>demand_supply</td>\n",
              "      <td>[0.0001709611, -0.07566928, -0.003781176, -0.0...</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>73.223686</td>\n",
              "      <td>-73.223686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>extend_concession</td>\n",
              "      <td>[-0.0007263941, -0.07511506, -0.004727404, -0....</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>137.519275</td>\n",
              "      <td>-137.519275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 words  ... sentiment_coeff\n",
              "535                           sub_plan  ...     -148.799847\n",
              "536                             across  ...     -167.555526\n",
              "537                                w_e  ...      -80.940936\n",
              "538                                  f  ...      -66.701085\n",
              "539  charger/adapter_headsets/speakers  ...      -99.961028\n",
              "540                            set_top  ...     -149.250401\n",
              "541                   repair_condition  ...     -116.650753\n",
              "542                       pile_epingle  ...     -106.838911\n",
              "543                      demand_supply  ...      -73.223686\n",
              "544                  extend_concession  ...     -137.519275\n",
              "\n",
              "[10 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DczBr1CBogs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}