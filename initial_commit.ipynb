{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Capstone_AMZ.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solharsh/Capstone_Sentiment_Analysis/blob/master/initial_commit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLNCwnXmYr9U",
        "colab_type": "text"
      },
      "source": [
        "# Importing necessary libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6g1zvTXbdmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "cc8e47a2-84ba-4436-f9e9-1da02e7e8eeb"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io9rT3UbYr9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "03c19778-23c8-4377-83e0-7f5ab79bea28"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from spacy import displacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('words')\n",
        "from nltk.text import Text\n",
        "import string, re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "string.punctuation\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import spacy\n",
        "#nlp = spacy.load('en')\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob.taggers import NLTKTagger\n",
        "#fromvaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from tqdm import trange\n",
        "#analyzer = SentimentIntensityAnalyzer()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyDY4fsVYr_v",
        "colab_type": "text"
      },
      "source": [
        "# Reading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Y_48rdbhYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "5596bf08-89c5-4c59-85a8-9a6a0ffdf20a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJKqrTtLYr_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "5e5bd69e-1f99-4566-9486-d35ecb558930"
      },
      "source": [
        "#path of the directory with data\n",
        "path = r'/content/drive/My Drive/Capstone Project - NLP/NLP Assignment RAW/Text'\n",
        "#Getting the list of files contained in the above path\n",
        "files = os.listdir(path)\n",
        "#printing the result to see the file names\n",
        "files"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['201203 PM.txt',\n",
              " '201407 AJ.txt',\n",
              " '201502 AJ.txt',\n",
              " '201902 PG Int.txt',\n",
              " '201907 NS.txt',\n",
              " '201702 AJ.txt',\n",
              " '201802 AJ.txt',\n",
              " '201002 PM.txt',\n",
              " '201602 AJ.txt',\n",
              " '201102 PM.txt',\n",
              " '201302 PC.txt',\n",
              " '201402 PC Int.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3ys9Q6eYr_-",
        "colab_type": "text"
      },
      "source": [
        "These are the files that are present in the Text folder. Lets read all the data in these files and keep it in an object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lztQgQwOYsAA",
        "colab_type": "text"
      },
      "source": [
        "### Creating a list containing all the speeches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INR49eZ-YsAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_speech_list = []\n",
        "for f in files:\n",
        "    file_full_path = os.path.join(path, f)\n",
        "    with open(file_full_path, encoding=\"ISO-8859-1\") as f:\n",
        "        all_speech_list.append(f.read().strip())        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH07jd18YsAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "56d26a63-720b-4029-bb7f-a79df4a27709"
      },
      "source": [
        "print('We have the data for {} files'.format(len(all_speech_list)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have the data for 12 files\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKi9n4gEYsAY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking the data with one speech\n",
        "all_speech_list[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr8MnRrEYsAh",
        "colab_type": "text"
      },
      "source": [
        "## Getting names of Speakers for the respective speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91GiOEPMYsAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Budget  2010-2011\\n\\n \\n\\nSpeech  of\\n\\nPranab Mukherjee\\n\\nMinister of Finance\n",
        "pattern = r'Speech\\s*of\\s*(.*\\s.*)\\s*Minister'\n",
        "found = re.findall(pattern, ' '.join(all_speech_list))\n",
        "speakers = [w.replace('\\n', '').strip() for w in found]\n",
        "print('Names of speakers are as follows:')\n",
        "print(*speakers,sep='\\n') \n",
        "print(\"Total Speakers: \", len(speakers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1a-1GrLYsBU",
        "colab_type": "text"
      },
      "source": [
        "## Getting respective speech's date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRP76TplYsBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Budget  2010-2011\\n\\n \\n\\nSpeech  of\\n\\nPranab Mukherjee\\n\\nMinister of Finance\\n\\n \\n\\nFebruary  26,  2010\\n\\n \\n\\nMadam\n",
        "speech_dates = []\n",
        "pattern = r'[A-Z]\\w+\\s*\\d{1,2}\\s*,\\s*\\d{4}'\n",
        "for speech in all_speech_list:\n",
        "    found = re.search(pattern, speech)\n",
        "    if found: \n",
        "        speech_date = speech[found.start():found.end()].replace('\\n','').strip()\n",
        "        speech_dates.append(speech_date)\n",
        "print('Respective Dates:')\n",
        "print(*speech_dates, len(speech_dates), sep='\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJsoKiL_YsBn",
        "colab_type": "text"
      },
      "source": [
        "Creating Dataframe for Speaker Name, Speech Date and Speeches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWkV_II3YsBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame({'Speaker_Name':speakers,'Date_Of_Speech':speech_dates,'Speech':all_speech_list})\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89RpOAxSYsB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['parsed'] = df.Speech.apply(nlp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpvzGI0eYsCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Document Count\")\n",
        "print(df.groupby('Speaker_Name')['Speech'].count())\n",
        "print(\"Word Count\")\n",
        "df.groupby('Speaker_Name').apply(lambda x: x.Speech.apply(lambda x: len(x.split())).sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ioqjiVqYsC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shape of dataset\n",
        "print(\"Input data has {} rows and {} columns\".format(len(df), len(df.columns)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgTXBP2uYsDF",
        "colab_type": "text"
      },
      "source": [
        "#### There are many negation words like haven't, shouldn't, isn't and so on which we need for further sentiment analysis. \n",
        "#### Decontrating those to make sure we have \"not\" word for further usage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdz47UCSYsDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "df['Speech'] = df['Speech'].apply(lambda x: decontracted(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBSqJeqEYsDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stance_features_extraction(df):\n",
        "    df['word_count'] = df['Speech'].apply(lambda x: len((re.sub(' +',' ',re.sub(r'[^a-zA-Z0-9 ]', '', x))).strip().split(' ')))\n",
        "    df['negation'] = df['Speech'].apply(lambda x: any(n in x for n in ['no', 'not', \"n\\'t\"]))\n",
        "    df['length'] = df['Speech'].apply(len)\n",
        "    df['has_url'] = df['Speech'].apply(lambda x: bool(re.search('http(s)?://', x)))\n",
        "    df['quest_mark'] = df['Speech'].apply(lambda x: x.count('?'))\n",
        "    df['excl_mark'] = df['Speech'].apply(lambda x: x.count('!'))\n",
        "\n",
        "    #df['speech_polarity'] =  df['Speech'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
        "    #df['speech_subjectivity'] =  df['Speech'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
        "    #sid = SentimentIntensityAnalyzer()\n",
        "    #df = df.join(df['Speech'].apply(lambda x: sid.polarity_scores(x)).apply(pd.Series))\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6FFUEvEYsDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = stance_features_extraction(df)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhxEA_ldYsDl",
        "colab_type": "text"
      },
      "source": [
        "## Insights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEJ34YVrYsDm",
        "colab_type": "text"
      },
      "source": [
        "- Arun Jaitley has a lot to say in February 29, 2016 (longest speech of 24551 words)\n",
        "- On the other hand P. Chidambaram ran short of words on February 17, 2014 (shortest speech of 6564 words)\n",
        "- URL was found in first three speeches of Pranab Mukherjee which we will be removing in upcoming cleaning steps.\n",
        "- Not many question marks are there and therefore, we do not need to emphasize much on the sentiment associated with those questions and we can simply remove the punctuation. \n",
        "- The same goes for exclamation mark. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELfS5As4YsDm",
        "colab_type": "text"
      },
      "source": [
        "I am intrigued by the length of speeches by Nirmala Sitharaman and Arun Jaitley. Let's check it manually. \n",
        "\n",
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPf9QDnJYsDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NS_Speech = df['Speech'][11]\n",
        "NS_Speech"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4aVWnfRYsD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Speech'][7]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_rrxJrZYsD8",
        "colab_type": "text"
      },
      "source": [
        "This made me observe that the format of 11th Speech is as follows: \n",
        "    \n",
        "- The\n",
        "- recent \n",
        "- election \n",
        "- which \n",
        "- brought \n",
        "- us \n",
        "- to \n",
        "- this \n",
        "- august \n",
        "- House \n",
        "- today, \n",
        "\n",
        "i.e every word is written in next line instead of continuous sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rONQsRu_YsD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let's correct the format and making it similar to all other speeches. \n",
        "\n",
        "import re\n",
        "NS_Speech = NS_Speech.replace('\\n','')\n",
        "NS_Speech\n",
        "df['Speech'][11] = NS_Speech"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0uvKs3VYsEG",
        "colab_type": "text"
      },
      "source": [
        "The speech contains few continuos sentences without any space in between. For example: \n",
        "exportedintheprecedingfinancialyearVReductionincustomsdutyforDefencesector57AnyChapterSpecifiedMilitaryequipmentandtheirpartsimportedbyMinistryofDefenceorArmedforcesApplicablerateNilVIAdditionalrevenuemeasures587106Silver\n",
        "\n",
        "To fix this, we need lot of inspection and manual work. So, using below function just correcting the words that are capital. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUq4QsikYsEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def capital_words_spaces(str1):\n",
        "  return re.sub(r\"(\\w)([A-Z])\", r\"\\1 \\2\", str1)\n",
        "\n",
        "print(capital_words_spaces(df.Speech[11]))\n",
        "#print(capital_words_spaces(\"PythonExercises\"))\n",
        "#print(capital_words_spaces(\"PythonExercisesPracticeSolution\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBpwSL9oYsER",
        "colab_type": "text"
      },
      "source": [
        "Maximum length of speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDFnN2aDYsES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['length'].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODSfrqi-YsEa",
        "colab_type": "text"
      },
      "source": [
        "# Minimum length of speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BERA-3KYsEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['length'].min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bj04xsuYsEe",
        "colab_type": "text"
      },
      "source": [
        "Maximum word count in a speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk5lohBcYsEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['word_count'].max()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYqSm3KWYsEp",
        "colab_type": "text"
      },
      "source": [
        "Minimum word count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYT4DtdmYsEq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['word_count'].min()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3CZCuQqYsFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df[\"length\"].plot.bar()\n",
        "#plt.show()\n",
        "x = Speakers\n",
        "y = df[\"length\"]\n",
        "plt.figure(figsize=(16, 6),dpi=70)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Length of Speech')\n",
        "plt.bar(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggf0jOb-YsFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Speakers\n",
        "y = df[\"word_count\"]\n",
        "plt.figure(figsize=(16, 6),dpi=70)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Length of Speech')\n",
        "plt.bar(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_APek5tYsFf",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning text and StopWords removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixwzdV-vYsFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the necessary functions\n",
        "import nltk, re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# This is a helper function to map NTLK position tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Let's get a list of stop words from the NLTK library\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# These words are important for our problem. We don't want to remove them.\n",
        "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
        "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
        "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
        "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "# New stop word list\n",
        "stop_words = [word for word in stop if word not in excluding]\n",
        "\n",
        "snow = SnowballStemmer('english')\n",
        "\n",
        "def process_text(texts): \n",
        "    final_text_list=[]\n",
        "    for sent in texts:\n",
        "        filtered_sentence=[]\n",
        "        \n",
        "        sent = sent.lower() # Lowercase \n",
        "        sent = sent.strip() # Remove leading/trailing whitespace\n",
        "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
        "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
        "        sent = re.sub(r\"\\b[A-Z]{2,}\\b\", \"\", sent) #removing accronyms\n",
        "        sent = re.sub(r\"\\d+\",\"\",sent) #removing numbers\n",
        "        sent = re.sub(r'http\\S+', '', sent) #remove URLs\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        for w in word_tokenize(sent):\n",
        "            # We are applying some custom filtering here, feel free to try different things\n",
        "            # Check if it is not numeric and its length>2 and not in stop words\n",
        "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words) and (w not in string.punctuation) and (w in words or not w.isalpha()):  \n",
        "                # Stem and add to filtered list\n",
        "                filtered_sentence.append(snow.stem(w))\n",
        "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
        " \n",
        "        final_text_list.append(final_string)\n",
        "    \n",
        "    return final_text_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_NKNTdkYsFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Pre-processing the Speech field with use of Stemming\")\n",
        "df[\"Speech_Stem\"] = process_text(df[\"Speech\"].tolist()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrjWAQOSYsF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Speech_Stem\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY72EuolYsGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the necessary functions\n",
        "import nltk, re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "words = set(nltk.corpus.words.words())\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# This is a helper function to map NTLK position tags\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Let's get a list of stop words from the NLTK library\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# These words are important for our problem. We don't want to remove them.\n",
        "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
        "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
        "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
        "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "# New stop word list\n",
        "stop_words = [word for word in stop if word not in excluding]\n",
        "\n",
        "snow = SnowballStemmer('english')\n",
        "\n",
        "def process_text(texts): \n",
        "    final_text_list=[]\n",
        "    for sent in texts:\n",
        "        filtered_sentence=[]\n",
        "        \n",
        "        sent = sent.lower() # Lowercase \n",
        "        sent = sent.strip() # Remove leading/trailing whitespace\n",
        "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
        "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
        "        sent = re.sub(r\"\\b[A-Z]{2,}\\b\", \"\", sent) #removing accronyms\n",
        "        sent = re.sub(r\"\\d+\",\"\",sent) #removing numbers\n",
        "        sent = re.sub(r'http\\S+', '', sent) #remove URLs\n",
        "\n",
        "        # Tokenize the sentence\n",
        "        for w in word_tokenize(sent):\n",
        "            # We are applying some custom filtering here, feel free to try different things\n",
        "            # Check if it is not numeric and its length>2 and not in stop words\n",
        "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words) and (w not in string.punctuation) and (w in words or not w.isalpha()):  \n",
        "                # Stem and add to filtered list\n",
        "                filtered_sentence.append(wl.lemmatize(w))\n",
        "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
        " \n",
        "        final_text_list.append(final_string)\n",
        "    \n",
        "    return final_text_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RKEfAzEYsGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Pre-processing the Speech field with use of Lemmatization\")\n",
        "df[\"Speech_Lemma\"] = process_text(df[\"Speech\"].tolist()) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RkJEvVVYsGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Speech_Lemma\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0nLymsEYsGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stance_features_extraction(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RkxEiWxYsGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df['Speech_Stem'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63Au70K2YsGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(df['Speech_Lemma'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1AArQZ6YsG5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['word_count_stem'] = df['Speech_Stem'].apply(lambda x: len((re.sub(' +',' ',re.sub(r'[^a-zA-Z0-9 ]', '', x))).strip().split(' ')))\n",
        "df['negation_stem'] = df['Speech_Stem'].apply(lambda x: any(n in x for n in [' no ', ' not ', 'n\\'t ']))\n",
        "df['length_stem'] = df['Speech_Stem'].apply(len)\n",
        "df['has_url_stem'] = df['Speech_Stem'].apply(lambda x: bool(re.search('http(s)?://', x)))\n",
        "df['quest_mark_stem'] = df['Speech_Stem'].apply(lambda x: x.count('?'))\n",
        "df['excl_mark_stem'] = df['Speech_Stem'].apply(lambda x: x.count('!'))\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MW1FYvF8YsHI",
        "colab_type": "text"
      },
      "source": [
        "Removing the columns that are not required anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AstFDzw7YsHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.drop(['negation','has_url','quest_mark','excl_mark','negation_stem'],axis=1)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NCQuogQYsHN",
        "colab_type": "text"
      },
      "source": [
        "We can see that after cleaning, there are no URLs, question mark or exclamation mark and there is significant reduction in Word count and length too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcqIHrw2YsHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Speakers\n",
        "y = df[\"length_stem\"]\n",
        "plt.figure(figsize=(16, 6),dpi=70)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Length of Speech after Stemming')\n",
        "plt.bar(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDADNrijYsHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Speakers\n",
        "y = df[\"word_count_stem\"]\n",
        "plt.figure(figsize=(16, 6),dpi=70)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Word count of Speeches after Stemming')\n",
        "plt.bar(x,y)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhezZ3ARYsHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['word_count_lemma'] = df['Speech_Lemma'].apply(lambda x: len((re.sub(' +',' ',re.sub(r'[^a-zA-Z0-9 ]', '', x))).strip().split(' ')))\n",
        "df['negation_lemma'] = df['Speech_Lemma'].apply(lambda x: any(n in x for n in [' no ', ' not ', 'n\\'t ']))\n",
        "df['length_lemma'] = df['Speech_Lemma'].apply(len)\n",
        "df['has_url_lemma'] = df['Speech_Lemma'].apply(lambda x: bool(re.search('http(s)?://', x)))\n",
        "df['quest_mark_lemma'] = df['Speech_Lemma'].apply(lambda x: x.count('?'))\n",
        "df['excl_mark_lemma'] = df['Speech_Lemma'].apply(lambda x: x.count('!'))\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xbpD8FjYsI1",
        "colab_type": "text"
      },
      "source": [
        "We observe that the length has been increased but the word count remains same after lemmatization which is expected as the stemmer just trims the suffixes wherein lemmatizer gives the root form of the word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgAsy9KYYsI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Removing unnecessary columns\n",
        "df = df.drop(['negation_lemma','has_url_lemma','quest_mark_lemma','excl_mark_lemma'],axis=1)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG5DL6rgYsI7",
        "colab_type": "text"
      },
      "source": [
        "### Let's see the reduction in lenght and word count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiwBRKMHYsI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['reduction_wc'] = df['word_count_lemma'] - df['word_count']\n",
        "df['reduction_len'] = df['length_lemma'] - df['length']\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLNwk-wYsJT",
        "colab_type": "text"
      },
      "source": [
        "### Let's see the average word count and length of speeches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SplF3h_dYsJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Average word count of speeches:\")\n",
        "print(df['reduction_wc'].mean())\n",
        "print(\"Average length of speeches:\")\n",
        "print(df['reduction_len'].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxvH4-fpYsJa",
        "colab_type": "text"
      },
      "source": [
        "### Computing Bag of Words Features\n",
        "\n",
        "We are using binary features here. TF and TF-IDF are other options."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-u2LgmbYsJc",
        "colab_type": "text"
      },
      "source": [
        "In below lines of code, 12 speeches are preprocessed, tokenized and represented as a sparse matrix. \n",
        "\n",
        "By default, CountVectorizer does the following:\n",
        "\n",
        "- lowercases your text (we can set lowercase=false if we don’t want lowercasing)\n",
        "- uses utf-8 encoding\n",
        "- performs tokenization (converts raw text to smaller units of text)\n",
        "- uses word level tokenization (meaning each word is treated as a separate token)\n",
        "- ignores single characters during tokenization (words like ‘a’ and ‘I’)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNXrarbKYsJd",
        "colab_type": "text"
      },
      "source": [
        "After we cleaned our data, we will now transform our corpus to a numerical format to make it understandable by the machine. To do that we are going to use the CountVectorizer function from the sklearn library.\n",
        "\n",
        "At the same time we are going to use a functionality of CountVectorizer function that eliminates stop words. Another method to do remove stop words is by using the stopwords corpus from nltk library.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ug1Xa7QYsJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# Initialize the binary count vectorizer\n",
        "tfidf_vectorizer = CountVectorizer(binary=True,\n",
        "                                   #max_features=500 # Limit the vocabulary size\n",
        "                                  )\n",
        "# Fit and transform\n",
        "X_vectors = tfidf_vectorizer.fit_transform(df[\"Speech_Lemma\"].tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mm3WIK3YsJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNJkGMovYsJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_vectors.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqLZRjyyYsJ4",
        "colab_type": "text"
      },
      "source": [
        "We have 12 (rows) documents and 5965 unique words (columns)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irI97NQaYsJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To keep track of the speakers, we create a new column that is the concatenation of the date and speakers’s name.\n",
        "#This column will be the index of our term-document matrix.\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "tfidf_vectorizer = CountVectorizer(binary = True, stop_words = stop_words)\n",
        "data_cv = tfidf_vectorizer.fit_transform(df[\"Speech_Lemma\"].tolist())\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
        "data_dtm.index = df[['Speaker_Name','Date_Of_Speech']].apply(lambda x: ':'.join(str(s) for s in x), axis=1)\n",
        "data_dtm.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTdoQ3L9YsKM",
        "colab_type": "text"
      },
      "source": [
        "This large matrix gives us a numerical representation of our corpus by specifying the words used in each speech.\n",
        "\n",
        "At the end of the text pre-processing phase, we generated a numerical matrix that features the words used in each speech transcript of our corpus. Also called a document-term matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXmknMxiYsKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data_dtm.transpose()\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oou-5TrYsKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMKXhNEfYsKr",
        "colab_type": "text"
      },
      "source": [
        "### Calculating word frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E3fv1ipYsKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False).head(30)\n",
        "    top_dict[c] = list(zip(top.index, top.values))\n",
        "top_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sHmgOWpYsK5",
        "colab_type": "text"
      },
      "source": [
        "### Print the top 15 words said by each FM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEQqAl-mYsK9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for Speakers, top_words in top_dict.items():\n",
        "    print(Speakers)\n",
        "    print(', '.join([word for word, count in top_words[0:14]]))\n",
        "    print('---')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxf9SECVYsLP",
        "colab_type": "text"
      },
      "source": [
        "### Look at the most common top words --> add them to the stop word list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grz-zk6wYsLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 30 words for each FM\n",
        "words = []\n",
        "for Speakers in data.columns:\n",
        "    top = [word for (word, count) in top_dict[Speakers]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "        \n",
        "words #words has list of all top 30 words used by each FM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUDyoGjXYsLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5NOKpnVYsL0",
        "colab_type": "text"
      },
      "source": [
        "### Let's aggregate this list and identify the most common words along with how many routines they occur in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1qgT07nYsL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Counter(words).most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g8eSPXCYsL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's get a list of stop words from the NLTK library\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "# These words are important for our problem. We don't want to remove them.\n",
        "excluding = ['against', 'not', 'don', \"don't\",'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",\n",
        "             'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", \n",
        "             'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
        "             'needn', \"needn't\",'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \n",
        "             \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "# New stop word list\n",
        "stop_words = [word for word in stop if word not in excluding]\n",
        "stop_words\n",
        "len(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kchy9dTPYsME",
        "colab_type": "text"
      },
      "source": [
        "We next want to take out these common words from our base, because they don’t add relevant information to help us make distinctions between speeches.\n",
        "\n",
        "Our condition will be to remove words that appear frequently in more than half of the speeches.\n",
        "\n",
        "We add these frequent words to the list of stop words and update our document-term matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzLzijCkYsMO",
        "colab_type": "text"
      },
      "source": [
        "### If more than half of the FMs have it as a top word, exclude it from the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj0a80llYsMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_FM = [word for word, count in Counter(words).most_common() if count > 5]\n",
        "stop_words.extend(stop_words_FM)\n",
        "stop_words.extend(['Rs','n','I', '', 'http', 'indiabudget', 'nic' , 's', 'per' ,'Rs', \n",
        "                       'The' ,'This','year','cent','crore', 'propose', 'In', 'would', 'To', 'in', \n",
        "                       'also', 'lakh', 'Budget', 'years', 'A', 'duty', 'Government' ,'set', 'last' \n",
        "                       ,'It', 'percent', 'i', 'We', 'Madam', 'Speaker', 'sector', 'rise', 'present', \n",
        "                       'Union' , 'Budget','sector', 'th', 'time', 'proposed', 'service', 'crores', 'crore', 'year',\n",
        "                       'government', 'shall', 'rate', 'income',\"Arun\", \"Jaitley\", \"Budget\",'Minister','Speaker','Madam','Sir','dtype','Nmae','Clean',\n",
        "              'Speech','India','Ju','page','CONTENTS','quo','allow','Name','ne','words','Length','seek',\n",
        "              'scope','Pranab','Mukherjee','Chidambaram','Piyush','Goyal','Nirmala','Sitharaman',\n",
        "             'year','po','stan','cha','object','pre','re','hold','speecha','wi','touc','presented','bs',\n",
        "             'Budg','February','placed','eventuall','fu','far','positi','ear','provided','worl','facing',\n",
        "             'year','sections','ago','popu','different','Indian','morning','Indi','happens','vel','yea',\n",
        "             'said','let','gives','Nehru','Jawaharlal','remain','Anru','fi','VIII','VI','IX','VII','gr',\n",
        "             'iv','Secto','deo','new','item','iii','Chapter','non','ii','ho','litre','years','Fe',\n",
        "             'Naren','Shri','etc','adm','la','fina','august','month','July','july','Vivekananda','Swami',\n",
        "             'Finance','relating','relati', 'NPS','cri','House','gra','kol','Roa','IMF','RWA','aa','AAI',\n",
        "             'AAM','ab','AAR','abs','ACA','ad','ADR','AEC','AI','AIF','AMC','AP','APA','APT','ARC','ARCs',\n",
        "             'AVGC','ATUFS','ASPIRE','hands','know','time','tomorrow','today','Today','eighth','seventh',\n",
        "             'sure','day','eariler','agents','Note','means','given','Prime','run','Annex','write','found',\n",
        "             'example','rest','Ahead','Annexes','Act','intent','gave','member','present','Thiruvalluvar',\n",
        "             'present','joins','clear','taking','€¢','must','notfurtherworkedthansemi-manufactured'] )\n",
        "print((stop_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRA7u58MYsMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03-tRsekYsMr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rem_cust_stop(texts): \n",
        "    final_text_list=[]\n",
        "    for sent in texts:\n",
        "        filtered_sentence=[]\n",
        "        \n",
        "        # Tokenize the sentence\n",
        "        for w in word_tokenize(sent):\n",
        "            # We are applying some custom filtering here, feel free to try different things\n",
        "            # Check if it is not numeric and its length>2 and not in stop words\n",
        "            if(not w.isnumeric()) and (len(w)>2) and (w not in stop_words) and (w not in string.punctuation) and (w in words or not w.isalpha()):  \n",
        "                # Stem and add to filtered list\n",
        "                filtered_sentence.append(w)\n",
        "        final_string = \" \".join(filtered_sentence) #final string of cleaned words\n",
        " \n",
        "        final_text_list.append(final_string)\n",
        "    \n",
        "    return final_text_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b16fYMe9YsMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Pre-processing the Speech field to remove custom stop words\")\n",
        "df[\"Speech_Stem_Cust_Stop\"] = process_text(df[\"Speech_Lemma\"].tolist()) \n",
        "df['Speech_Stem_Cust_Stop']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-zAlbVpYsM1",
        "colab_type": "text"
      },
      "source": [
        "### Length and word count after removing custom stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOPpbZ7UYsM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['word_count_cust_stop'] = df['Speech_Stem_Cust_Stop'].apply(lambda x: len((re.sub(' +',' ',re.sub(r'[^a-zA-Z0-9 ]', '', x))).strip().split(' ')))\n",
        "df['length_lemma_cust_stop'] = df['Speech_Stem_Cust_Stop'].apply(len)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ITfpubjYsM6",
        "colab_type": "text"
      },
      "source": [
        "### Let's see the reduction in length and word count after custom stop words removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6RAypCPYsM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['reduction_wc_cust_stop'] = df['word_count_cust_stop'] - df['word_count_lemma']\n",
        "df['reduction_len_cust_stop'] = df['length_lemma_cust_stop'] - df['length_lemma']\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niVuDVKoYsNC",
        "colab_type": "text"
      },
      "source": [
        "### Whoa! This is around 80-90% reduction in length and word count. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mLb44W5YsND",
        "colab_type": "text"
      },
      "source": [
        "### Recreate document-term matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksw4CVfYYsNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovS8GjkhYsNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = CountVectorizer(stop_words=stop_words)\n",
        "data_cv = tfidf_vectorizer.fit_transform(df.Speech_Stem_Cust_Stop)\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
        "data_dtm.index = df[['Speaker_Name','Date_Of_Speech']].apply(lambda x: ':'.join(str(s) for s in x), axis=1)\n",
        "data_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBQ7RLlWYsNQ",
        "colab_type": "text"
      },
      "source": [
        "Corpus has been reduced from 5929 to 1317 with the new stopwords."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hFn9ntYsNR",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing data with Word Clouds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2aH1JBhYsNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "wc = WordCloud(stopwords=stop_words, background_color='white', colormap='Dark2', max_font_size=150, random_state=42)\n",
        "df.index = df[['Speaker_Name','Date_Of_Speech']].apply(lambda x: ':'.join(str(s) for s in x), axis=1)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [16, 6]\n",
        "spaeker_names = data.columns\n",
        "# Create subplots for each speech\n",
        "for index, speech in enumerate(data.columns):\n",
        "    \n",
        "        wc.generate(df.Speech_Stem_Cust_Stop[speech])\n",
        "        plt.subplot(3, 4, index+1)\n",
        "        plt.imshow( wc, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(spaeker_names[index])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-XFidkIYsNY",
        "colab_type": "text"
      },
      "source": [
        "It is pretty clear from the word cloud that people have there own set of vocabulary. \n",
        "\n",
        "- Pranab Mukherjee has used exemption, country, reduce, power and so on extensively.\n",
        "- P.Chidambaran has focussed more on energy, power, deficit and such words\n",
        "- Arun Jaitley has emphasized more on taxation, country, reductions, concessions in his speeches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fClNLq6VYsNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \" \".join(speech for speech in df.Speech_Stem_Cust_Stop)\n",
        "print (\"There are {} words in the combination of all speeches.\".format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1ehr2UKYsNj",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will wrap up the exploration phase by counting the number of unique words per speech."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpkAb8y5YsNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data_dtm.transpose()\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut6rH1GhYsNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Words counting\n",
        "unique_list = []\n",
        "for speech in data.columns:\n",
        "    uniques = data[speech].nonzero()[0].size\n",
        "    unique_list.append(uniques)\n",
        "unique_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--xx3ab8YsOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenating speaker name with date of speech\n",
        "Speakers = df[['Speaker_Name','Date_Of_Speech']].apply(lambda x: ':'.join(str(s) for s in x), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxRJ8mMpYsOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_words = pd.DataFrame(unique_list,columns=['unique_words'],index=Speakers_)\n",
        "#data_unique_sort = data_words.sort_values(by='unique_words')\n",
        "#data_unique_sort\n",
        "\n",
        "# Create a new dataframe that contains this unique word count\n",
        "data_words = pd.DataFrame(list(zip(Speakers, unique_list)), columns=['Finance Minister', 'unique_words'])\n",
        "data_unique_sort = data_words.sort_values(by='unique_words')\n",
        "data_unique_sort"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRbqQ44JYsOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = Speakers\n",
        "y = data_unique_sort.unique_words\n",
        "plt.figure(figsize=(16, 6),dpi=70)\n",
        "plt.xticks(rotation=90)\n",
        "plt.bar(x,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPcE_IbyYsOb",
        "colab_type": "text"
      },
      "source": [
        "# SENTIMENT ANALYSIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7t1FXtoYsOc",
        "colab_type": "text"
      },
      "source": [
        "## 1. VADER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwrt1ovsYsOd",
        "colab_type": "text"
      },
      "source": [
        "#### Sentiment of Entire Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSo5WP4fYsOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentiment_scores_speech(speech): \n",
        "     \n",
        "    # Create a SentimentIntensityAnalyzer object. \n",
        "    sid_obj = SentimentIntensityAnalyzer() \n",
        "   \n",
        "    # polarity_scores method of SentimentIntensityAnalyzer \n",
        "    # oject gives a sentiment dictionary. \n",
        "    # which contains pos, neg, neu, and compound scores. \n",
        "    sentiment_dict = sid_obj.polarity_scores(speech) \n",
        "       \n",
        "    print(\"Overall sentiment dictionary is : \", sentiment_dict) \n",
        "    print(\"First Speech was rated as \", sentiment_dict['neg']*100, \"% Negative\") \n",
        "    print(\"First Speech was rated as \", sentiment_dict['neu']*100, \"% Neutral\") \n",
        "    print(\"First Speech was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
        " \n",
        "     \n",
        "    # decide sentiment as positive, negative and neutral \n",
        "    if sentiment_dict['compound'] >= 0.05 : \n",
        "        print(\"Positive\") \n",
        "         \n",
        "    elif sentiment_dict['compound'] <= - 0.05 : \n",
        "        print(\"Negative\") \n",
        "   \n",
        "    else : \n",
        "        print(\"Neutral\") \n",
        "    return sentiment_dict['compound'] \n",
        "\n",
        "sentiment_scores_speech(df['Speech_Stem_Cust_Stop'][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "648E8JmvYsOm",
        "colab_type": "text"
      },
      "source": [
        "#### Vader Score for Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0lmut-EYsOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Taking just first speech\n",
        "#tokenized_text = nltk.word_tokenize(speech_corpus[0])\n",
        "\n",
        "text = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[0])\n",
        "text1 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[1])\n",
        "text2 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[2])\n",
        "text3 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[3])\n",
        "text4 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[4])\n",
        "text5 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[5])\n",
        "text6 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[6])\n",
        "text7 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[7])\n",
        "text8 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[8])\n",
        "text9 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[9])\n",
        "text10 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[10])\n",
        "text11 = nltk.word_tokenize(df.Speech_Stem_Cust_Stop[11])\n",
        "\n",
        "text_corpus = [text,text1,text2,text3,text4,text5,text6,text7,text8,text9,text10,text11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtzInSHYYsOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sent_word(text):\n",
        "    pos_word_list=[]\n",
        "    neu_word_list=[]\n",
        "    neg_word_list=[]\n",
        "\n",
        "\n",
        "    for word in text:\n",
        "        vs = analyzer.polarity_scores(word)\n",
        "        if vs[\"compound\"] >= 0.05 : \n",
        "            pos_word_list.append(word)\n",
        "            score = analyzer.polarity_scores(word)\n",
        "            #print(\"{:-<65} {}\".format(word, str(vs)))\n",
        "        elif vs[\"compound\"] <= - 0.05 : \n",
        "            neg_word_list.append(word)\n",
        "            score = analyzer.polarity_scores(word)\n",
        "            #print(\"{:-<65} {}\".format(word,  str(vs)))\n",
        "        else: \n",
        "            neu_word_list.append(word)\n",
        "            #print(\"{:-<65} {}\".format(word,  str(vs)))\n",
        " \n",
        "    #print(Speakers_[0])\n",
        "    print(\"Overall Count:\")\n",
        "    print(len(text))\n",
        "    print(\"Positve Count:\")\n",
        "    print(len(pos_word_list))\n",
        "    print(\"Neutral Count:\")\n",
        "    print(len(neu_word_list))\n",
        "    print(\"Negative Count:\")\n",
        "    print(len(neg_word_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvzAiOBNYsO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_sent_word(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJxnPCIMYsO4",
        "colab_type": "text"
      },
      "source": [
        "### Checking coverage of VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kyVo8tEYsO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for text in text_corpus:\n",
        "    print('-----------------------------------------------------------------')\n",
        "    get_sent_word(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siOW1zZeYsPF",
        "colab_type": "text"
      },
      "source": [
        "There are too many Neutral words which is expected from VADER. Let's explore what can be done about it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA-bD_4jYsPG",
        "colab_type": "text"
      },
      "source": [
        "### Unique Positive Words in First Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt_TGBw-YsPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "pos_word_counts = Counter(pos_word_list)\n",
        "print(\"Unique Positive Words: \",len(pos_word_counts))# unique positive words\n",
        "pos_word_counts.most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUOXPaYEYsPU",
        "colab_type": "text"
      },
      "source": [
        "### Unique Negative Words in First Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFUdLxAPYsPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_word_counts = Counter(neg_word_list)\n",
        "print(\"Unique Negative Words: \",len(neg_word_counts))# unique positive words\n",
        "neg_word_counts.most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMRI2UJlYsPY",
        "colab_type": "text"
      },
      "source": [
        "### Unique Neutral Words in First Speech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYbhSC3cYsPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neu_word_counts = Counter(neu_word_list)\n",
        "print(\"Unique Neutral Words: \",len(neu_word_list))# unique positive words\n",
        "neu_word_counts.most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzHwq2PcYsPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_cloud(wd_list):\n",
        "    stopwords = set(stop_words)\n",
        "    all_words = ' '.join([text for text in wd_list])\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stop_words,\n",
        "        width=1600,\n",
        "        height=800,\n",
        "        random_state=21,\n",
        "        colormap='Dark2',\n",
        "        max_words=50,\n",
        "        max_font_size=200).generate(all_words)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkzbK4FcYsPj",
        "colab_type": "text"
      },
      "source": [
        "### Word cloud of negative words given by VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDJw8jPxYsPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_cloud(neg_word_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5SSyY4MYsPp",
        "colab_type": "text"
      },
      "source": [
        "### Word cloud of positive words given by VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0VXYD0wYsPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_cloud(pos_word_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I7JSYeXYsP0",
        "colab_type": "text"
      },
      "source": [
        "### Word cloud of neutral words given by VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwsIMyuDYsP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_cloud(neu_word_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5pF4WblYsP6",
        "colab_type": "text"
      },
      "source": [
        "# 2. SENTIWORDNET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCsSOKt9YsP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# We need to download the 'punkt' package to use tokenizers\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMIjU3nyYsQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize stemmer. \n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "# Our sample sentences here.\n",
        "\n",
        "for word in words:\n",
        "    tokens = word_tokenize(df['Speech_Stem_Cust_Stop'][0])\n",
        "    for word in tokens:\n",
        "        lemma = wl.lemmatize(word)\n",
        "        synsets = list(swn.senti_synsets(lemma))\n",
        "        # if it is not in the sentiment library, pass. \n",
        "        if not synsets:\n",
        "            continue\n",
        "        # Pick the first result\n",
        "        synset = synsets[0]\n",
        "        print(synset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxaX1Ug3YsWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        " \n",
        " \n",
        "lemmatizer = WordNetLemmatizer()\n",
        " \n",
        " \n",
        "def penn_to_wn(tag):\n",
        "    \"\"\"\n",
        "    Convert between the PennTreebank tags to simple Wordnet tags\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        " \n",
        "def swn_polarity(text):\n",
        "    \"\"\"\n",
        "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
        "    \"\"\"\n",
        " \n",
        "    sentiment = 0.0\n",
        "    tokens_count = 0\n",
        " \n",
        "    raw_sentences = sent_tokenize(text)\n",
        "    for raw_sentence in raw_sentences:\n",
        "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
        " \n",
        "        for word, tag in tagged_sentence:\n",
        "            wn_tag = penn_to_wn(tag)\n",
        "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "                continue\n",
        " \n",
        "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "            if not lemma:\n",
        "                continue\n",
        " \n",
        "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
        "            if not synsets:\n",
        "                continue\n",
        " \n",
        "            # Take the first sense, the most common\n",
        "            synset = synsets[0]\n",
        "            swn_synset = swn.senti_synset(synset.name())\n",
        " \n",
        "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
        "            tokens_count += 1\n",
        " \n",
        "    # judgment call ? Default to positive or negative\n",
        "    if not tokens_count:\n",
        "        return 0\n",
        " \n",
        "    # sum greater than 0 => positive sentiment\n",
        "    if sentiment >= 0:\n",
        "        return 1\n",
        " \n",
        "    # negative sentiment\n",
        "    return 0\n",
        " \n",
        " \n",
        "print(swn_polarity(df['Speech'][0]))\n",
        "print(swn_polarity(df['Speech'][1]))\n",
        "print(swn_polarity(df['Speech'][2]))\n",
        "print(swn_polarity(df['Speech'][3]))\n",
        "print(swn_polarity(df['Speech'][4]))\n",
        "print(swn_polarity(df['Speech'][5]))\n",
        "print(swn_polarity(df['Speech'][6]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoodef6cYsWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "sentence = \"Iphone6 camera is awesome for low light \"\n",
        "token = nltk.word_tokenize(sentence)\n",
        "tagged = nltk.pos_tag(token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWK8PhaTYsWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def penn_to_wn(tag):\n",
        "    \"\"\"\n",
        "    Convert between the PennTreebank tags to simple Wordnet tags\n",
        "    \"\"\"\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_sentiment(word,tag):\n",
        "    \"\"\" returns list of pos neg and objective score. But returns empty list if not present in senti wordnet. \"\"\"\n",
        "\n",
        "    wn_tag = penn_to_wn(tag)\n",
        "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
        "        return []\n",
        "\n",
        "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
        "    if not lemma:\n",
        "        return []\n",
        "\n",
        "    synsets = wn.synsets(word, pos=wn_tag)\n",
        "    if not synsets:\n",
        "        return []\n",
        "\n",
        "    # Take the first sense, the most common\n",
        "    synset = synsets[0]\n",
        "    swn_synset = swn.senti_synset(synset.name())\n",
        "\n",
        "    return [swn_synset.pos_score(),swn_synset.neg_score(),swn_synset.obj_score()]\n",
        "\n",
        "\n",
        "\n",
        "words_data = word_tokenize(df['Speech'][0])\n",
        "# words_data = [ps.stem(x) for x in words_data] # if you want to further stem the word\n",
        "\n",
        "pos_val = nltk.pos_tag(words_data)\n",
        "neg_val = nltk.n\n",
        "senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
        "senti_val_neg = [get_sentiment(x,y) for (x,y) in neg_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDFd1CXPYsWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"pos_val is {pos_val}\")\n",
        "print(f\"senti_val is {senti_val}\")\n",
        "print(f\"neg_val is {neg_val}\")\n",
        "print(f\"senti_val is {senti_val}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpur3x1iYsW7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_data = word_tokenize(df['Speech'][0])\n",
        "words_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNH58cTXYsXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "breakdown = swn.senti_synset('breakdown.n.03')\n",
        "print(breakdown)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-g3LYs7YsXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(breakdown.pos_score())\n",
        "\n",
        "print(breakdown.neg_score())\n",
        "\n",
        "print(breakdown.obj_score())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ckDLbf9YsXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "happy = swn.senti_synsets('happy', 'a')\n",
        "all = swn.all_senti_synsets()\n",
        "print(all)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1KUS2trYsXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import flair\n",
        "flair_sentiment = flair.models.TextClassifier.load('en-sentiment')\n",
        "s = flair.data.Sentence(sentence)\n",
        "flair_sentiment.predict(s)\n",
        "total_sentiment = s.labels\n",
        "total_sentiment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcU5lx91YsX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}